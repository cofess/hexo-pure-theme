---
title: 《深度学习模型及应用详解》读书笔记 
date: 2019-12-01
mathjax: true
---

## 概述
难度还是有点大，不适合入门的，有些章节现在还是有很多看不懂，特别是第8，10，11，12章，后面有机会再继续看吧。

## TODO
1. 什么是`nce_loss`，它`sampled_softmax_loss`之间的区别。
2. dropout的起源和使用。
3. Batch Normalization具体的计算和推导
4. 《Query-drive Iterated Neighborhood Graph Search for Large Scale Indexing》论文研究一下
5. Rich-CDSSM、《Learning Deep Structured Semantic Models for Web Search using Clickthrough》
6. RapidScorer算法到底是个什么流程，有什么应用。

## 第二章 深度学习开源框架
深度学习的变化真的很快，在2015年的时候，tensoflow刚出来，theano还是深度学习框架的霸主，而如今，theano都已经销声匿迹，只剩下了tensorflow和pytorch在争霸了。

深度学习的框架中，主要有两种分布式架构，分别是**Parameter Server-Workers**和**All-Reduce**。

Parameter Server-Workers有两部分，分别是Parameter Server和Wokers，前者是用来存储和处理参数的，后者是用来负责处理数据的，如果是多个Parameter Server和Woker的话，就分别只负责一部分。Parameter Server从Work中获取数据计算后的梯度信息，然后更新参数，Work在处理的时候会从Parameter Server上获取参数数据进行数据处理，得到当前的数据梯度信息。

All-Reduce的模式是每台机器既负责计算，也负责模型参数的更新。把数据分成n份，分别在每台机器上计算，然后求一个平均的梯度，然后把所有机器上的梯度都按照这个参数进行更新。

## 第三章 多层感知机在自然语言处理方面的应用
自然语言处理中最简单的任务就是计算两个句子或者两篇文档之间的相似度。

要解决这个问题，首先要把词和文本在数学上表示出来。可以通过**bag-of-words**进行建模。假设一篇文档中有100不同的词，那么我可以用一个1000维的向量来表示某一个词，这个词出现的索引置为1，其余地方置为0，所有的单词就都能用1000维的向量唯一表示出来了。那么一篇文档的向量就可以通过把这篇文档中的每一个词向量相加得到，也就是如果这篇文章中出现了这个词，那么对应的索引上就置为1，没有出现就是0。

但是如果只是看有没有出现，还是有点粗糙，于是发明了**TF-IDF**算法。向量中不仅表示文档中是否出现了该词，更是表示该词在文档中的重要程度。对于任意一个词，以这个词在当前文档出现的次数为分子，以这个词在所有的文档中出现的次数的`$log$`值为分母，分子处以分母的值就是这个词的权重。这个也很好理解，但是很妙，如果某个词只在当前文档出现，说明这个词比较与众不同，能够反应这个文档和其他文档的差别。像“的”这种单词在什么文档中都能出现，它的价值就很小，信息量很少。

上面的做法有个缺点就是**无法反应语义**，比如“称赞”和“夸奖”其实表达的是一个意思，但是在词向量中就是完全不同了。

因此后来发展出了**Word2Vec**算法。

目前最常见的Word2Vec算法有两种，分别是CBOW(Continuous Bag-of-Words)算法和Skip-Gram算法。他们的本质是一样的，而且互为镜像。

对于一个单独的字来说是没有语义的，语义是体现在上下文中的，什么词跟在什么词后面是具有一定模式的，这种模式也产出了语义。我们可以通过一个上下文窗口，把当前的词和其前后的一个或多个词进行关联，构建出正样本数据及其标注。负样本进行抽样学习。

CBOW算法就是以上下文为输入，当前单词为输出，通过上下文预测当前词，而Skip-Gram算法是以当前单词为输入，上下文为输出，通过当前单词预测其上下文。

把当前单词转化为上下文就需要一个编码映射，这个就是我们要学习的模型。可以通过前面的简单的方法把单词映射为特征向量。

但是Word2Vec有一些缺陷，比如：1.固定短语没法识别，比如"Best Buy"其实是一家电商，但是如果放到上下文就不能理解了，可以先识别短语，然后把短语当成一个整体。2.有些词词性完全不同，但是它的上下文一样，比如"It was a bad weather yesterday"和"It was a good weather yesterday"。3.一词多义的问题很难解决。4.当词在语料中出现的次数很少时就不能很好的学习到高维信息了，有些人提出了基于词根的建模，一定程度上可以缓解这个问题。

由于负样本的数量特别多，所以有一种方法是负样本取样，我之前一直理解的就是我直接选取一个正样本，然后选取几个负样本，然后把正样本和负样本合在一起计算softmax不就可以了嘛？但是好像这种做法有问题。照理说应该随机采样。书中参考论文《Distributed Representations of Words and Phrases and their Compositionality》得到了采用采样的方法计算的loss，但是原文我也没看出有这个公式的推导。

$$
J_k(\theta) = \log(1 + \exp^{(-z^1)}) + \sum_n\log(1 + \exp^{z^{i_n}})
$$

其中`$i_n \in \{0,1,...,9\},i_n \ne 1$`，`$i_n=1$`是我们的正样本。

虽然论文中没有推公式，但是我们理解起来没有那么困难。其实就是让正样本的`$\exp^{(-z^1)}$`越大，那么loss越小，而负样本的`$\exp^{z^{i_n}}$`值越大，那么造成loss越大，这样让正样本的概率越来越大。

[示例代码](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py#L205)中提到，这里用了`tf.nn.nce_loss`而没有用`tf.nn.sampled_softmax_loss`，这块还没有仔细看。


## 第四章 卷积神经网络在图像分类中的应用
文中提到，引入ReLU激活函数后，可以减少梯度消失的影响，但是会带来另一个问题，有可能存在很少被激活的“死节点”。这个原因应该是小于0的部分梯度直接变成0造成的，但是，说明这部分的参数就是没用的啊，不可能参数都利用起来吧，就算是用sigmoid或者tanh激活函数，应该也会存在这种问题，只不过那种表现是梯度很小。

局部响应归一化(Batch Normalization)和仿生学中临近的非常活跃的神经元之间存在竞争机制，其实Normalization的操作很多地方都有，只不过这里是在特征图之间进行。

dropout最开始提出来是为了能够融合多个模型？这个没有看过具体的论文不太清楚。那么dropout应该用在哪一层呢？最后？还是可以用在中间。


## 第五章 递归神经网络
LSTM算法在2003年就已经在语音识别领域大有作为了，但是深度学习在2010年以后才真正普及，这是什么道理？

RNN和普通的深度学习网络不一样的地方就在于它有“记忆”的功能，对于普通的神经网络而言，不同的输入之间是无关的，其他的输入信息不会作用到另一个输入中，而RNN就把这个关系加上了。不同的输入其参数是共享的，并不是不同的模型，这个思维有点像在《python深度学习》里面提到的，把句子正向反向都输入到网络中进行处理，因为处理句子的模型参数应该是一套而不是两套，所以正向句子和反向句子输入的是一个模型，然后在后面把这两个输出连接到一起，这个也是同理，所有的语音应该是用同一套模型参数去处理，只不过RNN可以把之前的信息也加进来。

双向递归神经网络(Bidirectional Recurrent Neural Network, BRNN)和长短期记忆模型(Long Short-Term Memory, LSTM)是RNN的两种变种。

BRNN是一个信息可以是正向传入，也可以反向传入，这和上上段的思维是一样的。

LSTM是解决普通的RNN不能有效建模长距离依存关系的问题。下面一张图可以说是非常清晰的解释了什么是LSTM，比书上讲的那么多门清晰的多的多了。

<p align=center><img src="https://joeltsui-blog.oss-cn-hangzhou.aliyuncs.com/LSTM3-chain.png" alt="LSTM" title style/>

## 第六章 DeepIntent模型在信息检索领域的应用
普通的RNN可以接收一系列的输入，最后输出一个结果，当然中间也有结果输出，只是在最后的时候这部分很难用上，有时候我们要叠加LSTM模块，前一个模块的所有输出都引入到后一个模块上。还有另一种做法就是引入**注意力机制**，把中间的所有输出也都加上，然后通过一个权重向量得到一个输出。

我们在表达的时候其实是有重点的，有些词语很重要，有些词语去了也没有关系，词语的重要程度是不同的，因此引入注意力机制。

注意力机制的整个流程我们可以总结为下。

我们先假设一种场景，用户输入一个句子`$X=\{ x_1, x_2,...,x_t \}$`，然后我们要根据用户的输入要给他推荐广告。其中`$x_t  \in \mathbb{R}^V$`，这里的`$V$`表示的是向量的纬度，表示词典中不同词的个数，然后通过one-hot方法把原始输入映射成特征向量。

有了特征向量之后，我们先进行一层特征提取，通过embedding层对原始特征向量进行变换。

$$
e_t = W_{emb}x_t, s.t. W_{emb} \in \mathbb{R}^{d_{emb}\times V}
$$

其中的`$d_{emb}$`就是生成的嵌入式向量的大小。

经过嵌入式层的操作以后，就生成了`$\{e_1, e_2, ... , e_t\}$`的特征向量。

通过简单的RNN的网络，我们可以有以下操作。

$$
h_t = H(e_t, h_{t-1})
$$

其中的`$H$`就是RNN的映射权重，他是一个非线性的映射。可以有多种选择。每一个输入都能得到一个输出，到现在就有输出`$\{h_1, h_2, ... ,h_t \}$`.

所谓的注意力机制就是对于每一个输出都有一个权重，最后得到一个输出.

$$
h = \sum_{t=1}^{T}\alpha_th_t, \alpha_t = \frac{\exp^{s(h_t;\theta)}}{\sum_{t=1}^T(\exp^{s(h_t;\theta)})}
$$

这个`$\alpha_t$`就是我们的权重，这个权重通过`$s(h_t;\theta)$`得到概率分布，这就是所说的注意力模型网络。整个过程就是如下图所示了。

<p align="center"><img src="https://joeltsui-blog.oss-cn-hangzhou.aliyuncs.com/IMG_6193.jpg" alt="RNN" title style>

**在经过注意力模型`$s(h_t;\theta)$`后输出了一系列的值，这个值经过上面的公式计算变成权重。但是直接输出的值还经过了一道加工，就是把所有的值都减去了最大值，这样保证所有的值都是小于等于0，如果其中的最大值比较大，按照指数增长的趋势，可能会造成溢出。**

但是整个流程下来，loss怎么计算其实还不是很清晰。

输入用户的输入，就是类似于图像中的像素，是一系列的特征，然后每经过一次RNN的网络，它输出的不是一个数值，而是一个特征向量，通过注意力模型得到的分数是每一个分量的分数，最后经过权重计算后得到的也是一个向量，用这个向量和广告得到的特征向量进行比较，可以得到两个特征向量的相似度。在我们平时的计算中，一个例子进行就有负样本，但是我们这里是要通过输入才有负样本，也就是一个batch输入既有正样本也有负样本，然后把这个看成一体计算loss。


## 第七章 图像识别及在广告搜索方面的应用

从特征提取模型中提取出1024维的特征向量，但是这个特征向量还是很长，因此还需要把特征向量进行压缩，压缩的方法可以通过PCA或者Rich-CDSSM算法提取成64维的特征向量。然后通过ANN（近似最邻近）来搜索两个向量之间的相似度。找出和query图像相似的内容。

书中也提到了NGS（邻域图搜索，Query-drive Iterated Neighborhood Graph Search for Large Scale Indexing）。这个可以参考一下用于向量的搜索。

Rich-CDSSM的算法到底是什么

## 第八章 Seq2Seq模型在聊天机器人中的应用
最重要的部分一点代码也没有，很多问题也根本没有说清楚。

## 第九章 word2vec的改进：fastText模型
word2vec中我们通过单词的n-gram方法构建输入输出对，但是会遇到生僻词、未知词，这个就没办法了。但是英文单词中很多单词是由词根等构成的，我们可以在单词的层面应用n-gram方法，这个就是*subword*了。每一个单词由多个特征向量构成，每一个特征向量就是我们的subword。

本章最重要的内容就是**分层softmax**算法，其实我在实际中就遇到过，在求解softmax的值的时候，如果维度太大，会导致计算机内存不够算不下来，这里提出了分层的softmax算法。

所谓的分层softmax就是对所有的类构建成树状结构，把softmax的多分类变成sigmoid二分类。

## 第十章 生成对抗网络



## 第十三章 深度学习的下一个浪潮
我觉得未来的深度学习还是得在强化学习上面，其他方面也肯定大有作为，想想看互联网刚出来的时候不过是应用在很小范围内，但是现在几乎生活的方方面面都是计算机和互联网，未来深度学习或者机器学习会普及到生活的方方面面，显然现在还远远没有达到。